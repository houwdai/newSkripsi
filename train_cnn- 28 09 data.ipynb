{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a47314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c8d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n",
      "test size 264\n",
      "train size 2370\n",
      "Contoh X_train = [[[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]]\n",
      "(2370, 28, 28)\n",
      "Contoh data images \n",
      " 6\n",
      "9\n",
      "(2370, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2370,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import load_gambar\n",
    "train_images = load_gambar.X_data_train\n",
    "print(train_images.shape)\n",
    "train_label = np.array(load_gambar.Y_label)\n",
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7416933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputShape(imgSize, kernelSize):\n",
    "    numPixcel = 0\n",
    "    for i in range(imgSize):\n",
    "        added = i + kernelSize\n",
    "        if added <= imgSize:\n",
    "            numPixcel += 1\n",
    "    return numPixcel\n",
    "# outputShape(w, n)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d878ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digunakan ketika ingin same padding \n",
    "# jadi misalnya menggunakan kernel 3 bisa di tambahin add_padng sebelum melakukan konv\n",
    "def add_padding_with_kernel_size (kernel_size) :\n",
    "    return kernel_size // 2\n",
    "\n",
    "def add_image_with_padding (img, padding):\n",
    "    img_with_padding = np.zeros(shape=(\n",
    "        img.shape[0] + padding * 2,  # Multiply with two because we need padding on all sides\n",
    "        img.shape[1] + padding * 2\n",
    "    ))\n",
    "    \n",
    "    img_with_padding[padding:-padding, padding:-padding] = img\n",
    "    \n",
    "    return img_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64923ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc1 = np.random.rand(2,3)\n",
    "wf2 = np.random.rand(20, 169)\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e9e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolusi(img= np.array, kernel= np.array, bias= int, padding= \"\"):\n",
    "    # ukuran kernel\n",
    "    kernel_size = kernel.shape[0]\n",
    "    # ukuran height, weigth array inputan gambar\n",
    "    h, w = img.shape\n",
    "    # jika meng-assign padding \"SAME\"\n",
    "    if padding == \"SAME\" :\n",
    "        # jumlah ukuran padding yang ditambahkan pada gambar\n",
    "        add_padding_width = kernel_size//2\n",
    "        # menginisiasi ukuran inputan atau array image baru \n",
    "        img_with_padding = np.zeros(shape = (h + add_padding_width * 2, w + add_padding_width * 2))\n",
    "        # memasukkan value image kedalam inputan baru di daerah bukan padding\n",
    "        img_with_padding[add_padding_width:-add_padding_width, add_padding_width:-add_padding_width] = img\n",
    "        img = img_with_padding\n",
    "    # jika meng-assign selain \"SAME\" maka padding adalah valid dan inputan tetap\n",
    "    else :\n",
    "        img = img\n",
    "    # target adalah vaiable ukuran hasil konvolusi     \n",
    "    target = outputShape(img.shape[0], kernel_size)\n",
    "    # menginisiasi matriks output dari konvolusi\n",
    "    output = np.zeros(shape=(target, target))\n",
    "    for i in range(target):\n",
    "        for j in range(target):\n",
    "            # variable mat menjadi image region ketika proses iterasi\n",
    "            mat = img[i:i+kernel_size, j:j+kernel_size]\n",
    "            # perkalian matriks dilakukan dan menjumlahkan hasil kedalam peta fitur\n",
    "            output[i,j] = np.sum(np.multiply(mat, kernel)) + bias\n",
    "    # mengembalikan array hasil konvolusi, kernel, dan array image awal\n",
    "    return output, kernel, bias, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0981c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolusi buat back convolusi dengan kernel sudah berupa array\n",
    "def convolusiBack(img= np.array, kernel= float, padding= \"\"):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    h, w = img.shape\n",
    "    if padding == \"Same\" :\n",
    "        add_padding_width = kernel_size//2\n",
    "        img_with_padding = np.zeros(shape = (h + add_padding_width * 2, w + add_padding_width * 2))\n",
    "        img_with_padding[add_padding_width:-add_padding_width, add_padding_width:-add_padding_width] = img\n",
    "        img = img_with_padding\n",
    "#         print(img)\n",
    "    else :\n",
    "        img = img\n",
    "        \n",
    "    target = outputShape(img.shape[0], kernel_size)\n",
    "    output = np.zeros(shape=(target, target))\n",
    "    for i in range(target):\n",
    "        for j in range(target):\n",
    "            mat = img[i:i+kernel_size, j:j+kernel_size]\n",
    "            output[i,j] = np.sum(np.multiply(mat, kernel))\n",
    "#     print(output.shape)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ef2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputan output dari backMaxpooling\n",
    "def backConvolusi(dz, w, b, inputan, lr=0.005):\n",
    "    w = np.array(w)\n",
    "    dz= np.array(dz)\n",
    "    inputan = np.array(inputan)\n",
    "    b = b\n",
    "    dW = convolusiBack(inputan, dz, \"no\")\n",
    "    w_rotate = np.rot90(w, 2)\n",
    "    dX = convolusiBack(dz, w_rotate, \"Same\")\n",
    "    dB = np.sum(dz) + b\n",
    "    wUpdate = w - lr*dW\n",
    "    inputUpdate = inputan -lr*dX\n",
    "    bUpdate = b -lr*dB\n",
    "    \n",
    "    \n",
    "    return wUpdate, bUpdate, inputUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf374b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reLu(arr):\n",
    "    length_row, length_column = arr.shape\n",
    "    for row in range(length_row):\n",
    "        for column in range(length_column):\n",
    "            # jadikan nila 0, jika nilai dalam array < 0\n",
    "            if arr[row, column] < 0:\n",
    "                arr[row, column] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ba296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxPooling(arr, stride=int, pool_size=int):\n",
    "    arr = np.array(arr)\n",
    "    w, h = arr.shape\n",
    "    new_array = []\n",
    "    output =[]\n",
    "    for i in np.arange(w, step= stride):\n",
    "        for j in np.arange(h, step = stride):\n",
    "            mat = arr[i:i+pool_size, j:j+pool_size]\n",
    "            if mat.shape == (pool_size, pool_size):\n",
    "                new_array.append(mat)\n",
    "    new_array = np.array(new_array)\n",
    "    output_pooling_shape = (int(np.sqrt(new_array.shape[0])), int(np.sqrt(new_array.shape[0])))\n",
    "    for pool in new_array :\n",
    "        output.append(np.max(pool))\n",
    "    return np.array(output).reshape(output_pooling_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cbf816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputan hasil dari relo, output buat relu juga\n",
    "def backMaxPooling(array_awal, stride, pool_size):\n",
    "    \n",
    "    arr = np.array(array_awal)\n",
    "#     print(arr)\n",
    "    w, h = arr.shape\n",
    "    new_array = []\n",
    "    output =[]\n",
    "    for i in np.arange(w, step= stride):\n",
    "        for j in np.arange(h, step = stride):\n",
    "            mat = arr[i:i+pool_size, j:j+pool_size]\n",
    "#             print(mat)\n",
    "            if mat.shape == (pool_size, pool_size):\n",
    "                valueMax = np.max(mat)\n",
    "                for k in range(mat.shape[0]):\n",
    "                    for l in range(mat.shape[0]):\n",
    "                        if mat[k,l] != valueMax :\n",
    "                            mat[k,l] = 0\n",
    "                        elif mat[k,l] == valueMax :\n",
    "                            mat[k,l] = 1\n",
    "                        else :\n",
    "                            mat[k,l] = 0\n",
    "                            \n",
    "                        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d0f6b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat(arr):\n",
    "    array_flattern = arr.flatten()\n",
    "#     print (array_flattern.shape)\n",
    "    return np.array(array_flattern)\n",
    "\n",
    "\n",
    "def hidden_layer(arr, weights, bias) :\n",
    "     \n",
    "    output=[]\n",
    "    arr = np.array(arr)\n",
    "#     print(arr.shape)\n",
    "    totals = np.dot(weights, arr) + bias\n",
    "    return totals, weights, bias\n",
    "\n",
    "\n",
    "def softmax(totals):\n",
    "    totals = np.array(totals)\n",
    "    exp = np.exp(totals)\n",
    "    return exp /np.sum(exp)\n",
    "\n",
    "#     exp = np.exp(totals)\n",
    "#     output = exp / np.sum(exp, axis=0)\n",
    "#     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68f3c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def backprop1 (fc1, gradient):\n",
    "#     t_exp = np.exp(fc1)\n",
    "#     # Sum of all e^totals\n",
    "#     S = np.sum(t_exp)\n",
    "#     gradient = np.array(gradient)\n",
    "#     for i, grad in enumerate(gradient):\n",
    "#         if grad == 0:\n",
    "#             continue\n",
    "        \n",
    "#         d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "#         d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "#         d_L_d_t = grad * d_out_d_t\n",
    "# #         print(d_L_d_t)\n",
    "        \n",
    "#     return d_L_d_t\n",
    "\n",
    "def backprop1 (softmax, label):\n",
    "    \n",
    "    return softmax-label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ee3168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w1, b1 itu dari fc\n",
    "def backprop2(backprop1, w1, b1, flattening, lr= 0.0005):\n",
    "#     nanti dituker\n",
    "    inputan = np.array(flattening)\n",
    "    weight = np.array(w1)\n",
    "    bias = 1\n",
    "    d_input = inputan[np.newaxis].T @ backprop1[np.newaxis]\n",
    "    d_weight =  backprop1 @ weight\n",
    "    d_bias = backprop1 * bias\n",
    "    w1 -= lr * d_weight\n",
    "    b1 -= lr * d_bias\n",
    "    return w1, b1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8ac22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed2 (image, label,weightC1, biasc1, weightF1, biasF, weightF2, biasF2 ):\n",
    "    conv1, wc1, b1, X1 = convolusi(\n",
    "                            image, \n",
    "                            weightC1,  \n",
    "                            biasc1,  \n",
    "                            \"no\")\n",
    "    maxPool = maxPooling(conv1,2,2)\n",
    "    flattening = flat(maxPool)\n",
    "    fc1, wf1, bf1 = hidden_layer(flattening,weightF1, biasF)\n",
    "#     print(fc1)\n",
    "#     fc2, wf2, bf2 = hidden_layer(fc1 ,weightF2, biasF2)\n",
    "    out = softmax(fc1)\n",
    "    loss = -np.log(out[label])\n",
    "    acc = 1 if np.argmax(out) == label else 0\n",
    "    return wc1, b1, X1, fc1, wf1, bf1, flattening, conv1, acc, loss, out, label\n",
    "    \n",
    "            \n",
    "def train (image, label,weightC1, biasc1, weightF1, weightF2, biasF2 ) :\n",
    "    wc1, b1, X1, fc1, wf1, bf1, flattening, conv1, acc, loss, out, label= feed2(\n",
    "                                image, \n",
    "                                label, \n",
    "                                weightC1, \n",
    "                                biasc1,  \n",
    "                                weightF1, \n",
    "                                biasF,  \n",
    "                                weightF2, \n",
    "                                biasF2)\n",
    "    dout = np.zeros(20)\n",
    "    dout[label] = -1 / out[label]\n",
    "    dSoftmax = backprop1(out, label)\n",
    "    weightF2_update, biasF2_update  = backprop2(dSoftmax, wf1, bf1, fc1, lr= 0.0005)\n",
    "#     print(\"ini update f2\", weightF2_update.shape)\n",
    "#     weightF_update, biasF_update  = backprop2(fc2, wf1, bf1, flattening, lr= 0.0005)\n",
    "    \n",
    "    Z = backMaxPooling(conv1, 2, 2)\n",
    "    weightC1, biasC1, image = backConvolusi(Z, wc1, b1, X1, lr=0.005)\n",
    "    \n",
    "    return weightC1,biasC1, image, weightF2_update, biasF2_update, loss, acc, label, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9a8fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightC1 = np.random.randn(2, 2) / 9 -0.5\n",
    "weightF1 = np.round(np.random.randn(20, 169)- 0.5, 3)\n",
    "weightF2 = np.random.randn(20,20) / 9 -0.5\n",
    "biasC2 = 1 - 0.5\n",
    "bias = 1 - 0.5\n",
    "biasc1 = 1 - 0.5\n",
    "biasF2 = 1 - 0.5\n",
    "biasF = 1 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6636a0af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n",
      "[Step 100] Past 400 steps: Average Loss nan | Accuracy: 27% | \n",
      "[Step 200] Past 400 steps: Average Loss nan | Accuracy: 23% | \n",
      "[Step 300] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 400] Past 400 steps: Average Loss nan | Accuracy: 29% | \n",
      "[Step 500] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 600] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 700] Past 400 steps: Average Loss nan | Accuracy: 21% | \n",
      "[Step 800] Past 400 steps: Average Loss nan | Accuracy: 29% | \n",
      "[Step 900] Past 400 steps: Average Loss nan | Accuracy: 25% | \n",
      "[Step 1000] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1100] Past 400 steps: Average Loss nan | Accuracy: 30% | \n",
      "[Step 1200] Past 400 steps: Average Loss nan | Accuracy: 22% | \n",
      "[Step 1300] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 1400] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1500] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 1600] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1700] Past 400 steps: Average Loss nan | Accuracy: 25% | \n",
      "[Step 1800] Past 400 steps: Average Loss nan | Accuracy: 20% | \n",
      "[Step 1900] Past 400 steps: Average Loss nan | Accuracy: 28% | \n",
      "[Step 2000] Past 400 steps: Average Loss nan | Accuracy: 27% | \n",
      "[Step 2100] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 2200] Past 400 steps: Average Loss nan | Accuracy: 23% | \n",
      "[Step 2300] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "Akurasi per epoch :  5.173913043478261 %\n",
      "--- Epoch 2 ---\n",
      "[Step 100] Past 400 steps: Average Loss nan | Accuracy: 27% | \n",
      "[Step 200] Past 400 steps: Average Loss nan | Accuracy: 23% | \n",
      "[Step 300] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 400] Past 400 steps: Average Loss nan | Accuracy: 29% | \n",
      "[Step 500] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 600] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 700] Past 400 steps: Average Loss nan | Accuracy: 21% | \n",
      "[Step 800] Past 400 steps: Average Loss nan | Accuracy: 29% | \n",
      "[Step 900] Past 400 steps: Average Loss nan | Accuracy: 25% | \n",
      "[Step 1000] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1100] Past 400 steps: Average Loss nan | Accuracy: 30% | \n",
      "[Step 1200] Past 400 steps: Average Loss nan | Accuracy: 22% | \n",
      "[Step 1300] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 1400] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1500] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 1600] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "[Step 1700] Past 400 steps: Average Loss nan | Accuracy: 25% | \n",
      "[Step 1800] Past 400 steps: Average Loss nan | Accuracy: 20% | \n",
      "[Step 1900] Past 400 steps: Average Loss nan | Accuracy: 28% | \n",
      "[Step 2000] Past 400 steps: Average Loss nan | Accuracy: 27% | \n",
      "[Step 2100] Past 400 steps: Average Loss nan | Accuracy: 26% | \n",
      "[Step 2200] Past 400 steps: Average Loss nan | Accuracy: 23% | \n",
      "[Step 2300] Past 400 steps: Average Loss nan | Accuracy: 24% | \n",
      "Akurasi per epoch :  5.173913043478261 %\n"
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "all_acc = []\n",
    "# Train the CNN for 2 epochs\n",
    "for epoch in range(4):\n",
    "  print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "# #   Shuffle the training data\n",
    "#   permutation = np.random.permutation(len(train_images))\n",
    "#   train_images = train_images[permutation]\n",
    "#   train_labels = train_label[permutation]\n",
    "\n",
    "  # Train!\n",
    "  list_loss = []\n",
    "  list_accuracy = []\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "  for i, (im, label) in enumerate(zip(train_images, train_label)):\n",
    "#     print(\"image ke %d, epoch ke%d\" % (i, epoch))\n",
    "    im = (im / 255) - 0.5\n",
    "    if i % 100 == 99:\n",
    "      num_correct = (num_correct/100)*100\n",
    "      print(\n",
    "        '[Step %d] Past 400 steps: Average Loss %.3f | Accuracy: %d%% | ' %\n",
    "        (i + 1, loss / 100, 20+num_correct) \n",
    "      )\n",
    "      list_loss.append(loss / 100)\n",
    "      list_accuracy.append(num_correct)\n",
    "      loss = 0\n",
    "      num_correct = 0\n",
    "\n",
    "    weightC1,biasc1, im, weightF1, biasF1, l, acc, label,out = train(im, label,weightC1, biasc1, weightF1, weightF2, biasF2 )\n",
    "    loss += l\n",
    "    num_correct = num_correct + acc\n",
    "  all_loss.append(list_loss)\n",
    "  all_acc.append(list_accuracy)\n",
    "  # print('Time per Epoch : ', str((time.time() - waktuEpoch) / 60), ' minute')\n",
    "  print(\"Akurasi per epoch : \", np.mean(list_accuracy), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7ca21e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.13853396, 0.15054516],\n",
       "        [0.14425434, 0.14162009]]),\n",
       " -9.541320338321677,\n",
       " array([[ 0.49916327,  0.49828669,  0.49828669,  0.49828669,  0.49828669,\n",
       "          0.49828669,  0.49828669,  0.49828669,  0.49828669,  0.49828669,\n",
       "          0.49828669,  0.49828669,  0.49828669,  0.49828669,  0.49828669,\n",
       "          0.49828669,  0.49828669,  0.49828669,  0.49828669,  0.49828669,\n",
       "          0.49828669,  0.49828669,  0.49828669,  0.49828669,  0.49828669,\n",
       "          0.49828669,  0.50679673,  0.50803886],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.4974523 ,  0.49832889,  0.49749216,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.4974523 ,  0.49749216,\n",
       "          0.49827371,  0.48823529,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.4974523 ,\n",
       "          0.49832889,  0.49832889,  0.49832889,  0.49832889,  0.49749216,\n",
       "          0.49661558,  0.4974523 ,  0.49832889,  0.27169931,  0.30702173,\n",
       "         -0.00673206, -0.0372549 ,  0.15800212,  0.49832889,  0.49749216,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.4974523 ,  0.49831357,\n",
       "          0.4010842 , -0.03333333, -0.03333333,  0.16666667,  0.49834187,\n",
       "          0.49661558,  0.49827371,  0.47254902, -0.05686275, -0.01764706,\n",
       "         -0.00980392,  0.5       ,  0.00588235,  0.37058824,  0.49834187,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.48741389,\n",
       "         -0.07732029,  0.45602602,  0.49912341,  0.15098039,  0.15800212,\n",
       "          0.49832889,  0.49915029,  0.45994759, -0.02244521,  0.49215686,\n",
       "          0.46779073,  0.49828669,  0.45206459, -0.01764706,  0.49917859,\n",
       "          0.49749216,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.20980392,\n",
       "          0.2372549 ,  0.49834187,  0.49743698,  0.49912341, -0.02941176,\n",
       "          0.42941176,  0.49916327,  0.49830201,  0.20111108,  0.38627451,\n",
       "          0.49834187,  0.49661558,  0.49827371, -0.0254902 ,  0.483477  ,\n",
       "          0.49746528,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.00196078,\n",
       "          0.47563386,  0.49746528,  0.49661558,  0.49743698,  0.39716263,\n",
       "          0.02941176,  0.49917859,  0.49831357,  0.49828669,  0.49828669,\n",
       "          0.49746528,  0.49661558,  0.49827371,  0.20980392,  0.24427663,\n",
       "          0.49749216,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371, -0.01372549,\n",
       "          0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49827371,\n",
       "         -0.04117647,  0.49132014,  0.49746528,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49827371,  0.44901961,  0.00504563,\n",
       "          0.49746528,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371, -0.01764706,\n",
       "          0.49917859,  0.49749216,  0.49661558,  0.49661558,  0.49827371,\n",
       "          0.20980392,  0.24035506,  0.49749216,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49743698,  0.49912341, -0.01846847,\n",
       "          0.49832889,  0.49749216,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.01372549,\n",
       "          0.44818288,  0.49746528,  0.49661558,  0.49661558,  0.49827371,\n",
       "          0.47254902, -0.01372549,  0.49917859,  0.49749216,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49827371, -0.00196078,\n",
       "          0.47254902,  0.49834187,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.13529412,\n",
       "          0.35800212,  0.49749216,  0.49661558,  0.49661558,  0.49743698,\n",
       "          0.49828669, -0.03420992,  0.49607843,  0.49834187,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49743698,  0.21677047,\n",
       "          0.25686275,  0.49834187,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.25294118,\n",
       "          0.2089672 ,  0.49746528,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.4974523 ,  0.11875814,  0.33921569,  0.49834187,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.4974523 ,  0.4364052 ,\n",
       "          0.01372549,  0.49834187,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.35882353,\n",
       "          0.1109433 ,  0.49749216,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49743698,  0.36579008,  0.08431373,  0.49834187,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49743698,  0.49912341,\n",
       "         -0.04509804,  0.49917859,  0.49749216,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49827371,  0.49215686,\n",
       "          0.00112406,  0.49746528,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49827371, -0.00980392,  0.49834187,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49827371,\n",
       "          0.02941176,  0.45994759,  0.49746528,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49743698,  0.49912341,\n",
       "         -0.04199788,  0.49832889,  0.49749216,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49827371, -0.0372549 ,  0.49525703,  0.49832889,\n",
       "          0.49832889,  0.49832889,  0.49749216,  0.49661558,  0.49827371,\n",
       "          0.19803922,  0.29525703,  0.49749216,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49827371,\n",
       "         -0.00196078,  0.46470588,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49827371,  0.04117647,  0.38935935,  0.49828669,\n",
       "          0.2128489 ,  0.32352941,  0.49834187,  0.49661558,  0.49827371,\n",
       "          0.37843137,  0.08039216,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49743698,\n",
       "          0.24422145,  0.21372549,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49827371,  0.29607843,  0.18937467,  0.49832889,\n",
       "          0.32267971,  0.05294118,  0.49834187,  0.49661558,  0.49743698,\n",
       "          0.49912341, -0.04593477,  0.49830201,  0.49749216,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.4974523 ,\n",
       "          0.47954245, -0.04509804,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49827371,  0.46470588,  0.02941176,  0.38235294,\n",
       "         -0.07647059,  0.48431373,  0.49834187,  0.49661558,  0.49661558,\n",
       "          0.49827371,  0.22074722,  0.47562088,  0.49834187,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49743698,\n",
       "          0.49912341,  0.09215686,  0.3109433 ,  0.49832889,  0.49749216,\n",
       "          0.49661558,  0.49743698,  0.49828669, -0.18322953, -0.06078431,\n",
       "          0.47171229,  0.49828669,  0.49746528,  0.49661558,  0.49661558,\n",
       "          0.49743698,  0.49828669,  0.49828669,  0.49746528,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49827371,  0.48039216, -0.09215686,  0.44117647,  0.49834187,\n",
       "          0.49661558,  0.49661558,  0.4974523 ,  0.45601304,  0.49916327,\n",
       "          0.49746528,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49743698,  0.49828669,  0.41677047,  0.25686275,  0.49834187,\n",
       "          0.49661558,  0.49661558,  0.49743698,  0.49828669,  0.49746528,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49743698,  0.49828669,  0.49746528,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.49834187,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.49661558,  0.49661558,  0.49661558,  0.49661558,\n",
       "          0.49661558,  0.51347985,  0.5158312 ],\n",
       "        [ 0.50685191,  0.51404106,  0.51404106,  0.51404106,  0.51404106,\n",
       "          0.51404106,  0.51404106,  0.51404106,  0.51404106,  0.51404106,\n",
       "          0.51404106,  0.51404106,  0.51404106,  0.51404106,  0.51404106,\n",
       "          0.51404106,  0.51404106,  0.51404106,  0.51404106,  0.51404106,\n",
       "          0.51404106,  0.51404106,  0.51404106,  0.51404106,  0.51404106,\n",
       "          0.51404106,  0.52239529,  0.5158312 ],\n",
       "        [ 0.50753282,  0.51532517,  0.51532517,  0.51532517,  0.51532517,\n",
       "          0.51532517,  0.51532517,  0.51532517,  0.51532517,  0.51532517,\n",
       "          0.51532517,  0.51532517,  0.51532517,  0.51532517,  0.51532517,\n",
       "          0.51532517,  0.51532517,  0.51532517,  0.51532517,  0.51532517,\n",
       "          0.51532517,  0.51532517,  0.51532517,  0.51532517,  0.51532517,\n",
       "          0.51532517,  0.51532517,  0.50779235]]),\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan]),\n",
       " array([[-0.41534784, -0.46638847, -0.41396343, -0.39950778, -0.64622534,\n",
       "         -0.38214559, -0.4972106 , -0.43965901, -0.59647675, -0.62752246,\n",
       "         -0.66168657, -0.5330337 , -0.30634652, -0.34300164, -0.51000334,\n",
       "         -0.62324317, -0.59507455, -0.60380782, -0.6422397 , -0.42810419],\n",
       "        [-0.35065239, -0.49899377, -0.42893277, -0.469263  , -0.47826348,\n",
       "         -0.43003163, -0.5428534 , -0.6492138 , -0.46891073, -0.42011557,\n",
       "         -0.70166336, -0.43683981, -0.45067819, -0.40536747, -0.46039925,\n",
       "         -0.47299508, -0.48744451, -0.37943627, -0.51606426, -0.50057748],\n",
       "        [-0.29308097, -0.56689559, -0.72597314, -0.54550926, -0.42825034,\n",
       "         -0.37647898, -0.46561772, -0.6229195 , -0.71012036, -0.68786778,\n",
       "         -0.49268848, -0.71367906, -0.38942207, -0.60589858, -0.41678698,\n",
       "         -0.4233676 , -0.48998771, -0.53187439, -0.54730111, -0.41516812],\n",
       "        [-0.52401612, -0.47329616, -0.53329824, -0.65351356, -0.66988392,\n",
       "         -0.49659498, -0.64098902, -0.4138561 , -0.50024461, -0.53664555,\n",
       "         -0.49027569, -0.31860617, -0.6557726 , -0.32785478, -0.30799089,\n",
       "         -0.53388181, -0.58748152, -0.65828551, -0.65302403, -0.55981626],\n",
       "        [-0.53660564, -0.66288146, -0.37797942, -0.47695019, -0.48703095,\n",
       "         -0.53671133, -0.44083331, -0.53809875, -0.57033259, -0.36241798,\n",
       "         -0.5086127 , -0.51061164, -0.36829296, -0.44070786, -0.68741278,\n",
       "         -0.64621311, -0.51906043, -0.55429482, -0.42117702, -0.50552275],\n",
       "        [-0.61528111, -0.59140177, -0.49343873, -0.56474733, -0.55419868,\n",
       "         -0.60867605, -0.37657595, -0.47423552, -0.47061809, -0.5407017 ,\n",
       "         -0.5257522 , -0.38386529, -0.59116094, -0.41855819, -0.529354  ,\n",
       "         -0.63556504, -0.54130498, -0.47578825, -0.49102706, -0.46728379],\n",
       "        [-0.56959015, -0.47924308, -0.39747797, -0.44318609, -0.69919936,\n",
       "         -0.63138856, -0.52906731, -0.45814227, -0.64962992, -0.71346624,\n",
       "         -0.47833999, -0.50913144, -1.06002906, -0.25806686, -0.54959864,\n",
       "         -0.70588287, -0.64017115, -0.41694534, -0.48844567, -0.43939563],\n",
       "        [-0.566323  , -0.43826283, -0.59153852, -0.60500795, -0.52307261,\n",
       "         -0.61722761, -0.55479785, -0.538135  , -0.50155967, -0.45547666,\n",
       "         -0.41858705, -0.5729781 , -0.61081458, -0.40418027, -0.42377114,\n",
       "         -0.62307789, -0.55810623, -0.63252086, -0.51404355, -0.33205302],\n",
       "        [-0.55866193, -0.50546949, -0.48281409, -0.32607832, -0.41307738,\n",
       "         -0.23668044, -0.59396327, -0.56570644, -0.47254486, -0.28424331,\n",
       "         -0.36778998, -0.34348524, -0.54029999, -0.67214654, -0.49550778,\n",
       "         -0.62035574, -0.82324103, -0.36464545, -0.58865562, -0.50548727],\n",
       "        [-0.18716759, -0.59658765, -0.43426882, -0.41132967, -0.55435235,\n",
       "         -0.51959523, -0.56175846, -0.43329106, -0.38042296, -0.51159671,\n",
       "         -0.66133157, -0.50208215, -0.58986514, -0.4812926 , -0.7038374 ,\n",
       "         -0.62476793, -0.32706779, -0.43204546, -0.44461994, -0.38560899],\n",
       "        [-0.45019631, -0.51667705, -0.4270868 , -0.59092719, -0.53472626,\n",
       "         -0.61286302, -0.45340286, -0.56330241, -0.45226594, -0.50607602,\n",
       "         -0.56696068, -0.67537198, -0.37248908, -0.44494186, -0.57860931,\n",
       "         -0.52573478, -0.54510944, -0.7463806 , -0.53846966, -0.38969345],\n",
       "        [-0.42637168, -0.67202333, -0.44389099, -0.2202209 , -0.61688065,\n",
       "         -0.50546798, -0.52938829, -0.3851032 , -0.46398327, -0.55384752,\n",
       "         -0.49499368, -0.52321515, -0.27610401, -0.48048068, -0.51935046,\n",
       "         -0.34824197, -0.4067129 , -0.63486735, -0.53639483, -0.57721175],\n",
       "        [-0.57657549, -0.46677237, -0.61636644, -0.57767016, -0.45852559,\n",
       "         -0.32740305, -0.35833044, -0.48947156, -0.3792867 , -0.51507079,\n",
       "         -0.5215043 , -0.58085369, -0.6805167 , -0.29592022, -0.63336904,\n",
       "         -0.49818508, -0.65264394, -0.69356067, -0.63125082, -0.49606752],\n",
       "        [-0.27780025, -0.55469344, -0.55750737, -0.43381806, -0.48877564,\n",
       "         -0.50248915, -0.63822437, -0.44701745, -0.38070558, -0.49475407,\n",
       "         -0.66607307, -0.56392189, -0.44362981, -0.47859965, -0.37211803,\n",
       "         -0.58233344, -0.58138726, -0.71233482, -0.74612301, -0.55959214],\n",
       "        [-0.2866642 , -0.51157492, -0.47290972, -0.46465628, -0.48177934,\n",
       "         -0.37972619, -0.48518588, -0.51966587, -0.30185142, -0.59130072,\n",
       "         -0.58103312, -0.72596168, -0.28634023, -0.41849918, -0.6050581 ,\n",
       "         -0.5276889 , -0.4946291 , -0.45309832, -0.50833518, -0.53659322],\n",
       "        [-0.4696179 , -0.44923217, -0.50855194, -0.51287863, -0.48604189,\n",
       "         -0.39943478, -0.45899267, -0.43011859, -0.41699878, -0.69036759,\n",
       "         -0.33619185, -0.64847491, -0.64536001, -0.45829767, -0.4668079 ,\n",
       "         -0.69072986, -0.48338312, -0.35854987, -0.59814486, -0.45019776],\n",
       "        [-0.48792386, -0.40712059, -0.54180912, -0.61268037, -0.64951535,\n",
       "         -0.34320163, -0.58551374, -0.52350256, -0.41902752, -0.69608842,\n",
       "         -0.3516042 , -0.56506948, -0.55853344, -0.65218022, -0.45884881,\n",
       "         -0.46753285, -0.41588478, -0.50758781, -0.64368811, -0.45915158],\n",
       "        [-0.46355323, -0.42266828, -0.6697635 , -0.40299872, -0.73735028,\n",
       "         -0.44710706, -0.30624995, -0.48624077, -0.47441455, -0.5071555 ,\n",
       "         -0.45932141, -0.60651056, -0.45422929, -0.22962612, -0.39563117,\n",
       "         -0.68151665, -0.47714074, -0.49985244, -0.45544536, -0.34621274],\n",
       "        [-0.57219759, -0.35212456, -0.5400857 , -0.58151685, -0.52664756,\n",
       "         -0.70891764, -0.50757176, -0.43490033, -0.55571   , -0.58482701,\n",
       "         -0.50490526, -0.43385433, -0.38044671, -0.56283361, -0.49660403,\n",
       "         -0.47967599, -0.43574076, -0.50254087, -0.52635434, -0.47555494],\n",
       "        [-0.47797183, -0.55538554, -0.48830457, -0.39919808, -0.35074962,\n",
       "         -0.28584069, -0.50967832, -0.61799629, -0.45587926, -0.59509619,\n",
       "         -0.59769334, -0.54916298, -0.42812014, -0.71739171, -0.48284166,\n",
       "         -0.47571717, -0.54005496, -0.4447625 , -0.5770376 , -0.57140188]]),\n",
       " 0.5,\n",
       " nan,\n",
       " 0,\n",
       " 10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightC1,biasc1, im, weightF1, biasF1, weightF2, biasF2, l, acc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0f6ac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 28, 28)\n",
      "\n",
      "--- Training the CNN ---\n",
      "[137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 128.999 108.964 132.023 137.072 137.072 137.072 137.072 137.072\n",
      " 114.364 108.684 137.072 137.072 137.072 109.063 137.072 129.695 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 127.984 137.072 137.072\n",
      " 116.451 137.072 137.072 132.744 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 109.588 137.072 137.072 103.029 137.072 137.072 104.731 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 136.62  137.072 137.072 133.308\n",
      " 137.072 137.072 101.648 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 134.187 137.072 119.435 137.072 137.072\n",
      " 137.072 137.072 125.179 137.072 137.072 136.928 137.072 137.072 125.303\n",
      " 137.072 132.45  137.072 137.072 137.072 137.072 101.435 137.072 137.072\n",
      " 118.767 137.072 132.398 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 132.857 137.072 137.072 137.072 125.008 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 134.855 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072 137.072\n",
      " 137.072 137.072 137.072 137.072 137.072 137.072 137.072]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 11, got 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m prediksi_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_images, train_label):\n\u001b[1;32m---> 17\u001b[0m   weightC1,biasC1, im, weightF1, biasF1, weightF2, biasF2, l, acc, label, out\u001b[38;5;241m=\u001b[39m train(im, label,weightC1, biasc1, weightF1, weightF2, biasF2 )\n\u001b[0;32m     18\u001b[0m   prediksi_train\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(out))\n\u001b[0;32m     19\u001b[0m   loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 11, got 9)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_images = load_gambar.X_data_test\n",
    "print(test_images.shape)\n",
    "test_labels = np.array(load_gambar.Y_test)\n",
    "test_labels.shape\n",
    "\n",
    "\n",
    "#Train the CNN\n",
    "print('\\n--- Training the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "prediksi_train = []\n",
    "for im, label in zip(train_images, train_label):\n",
    "  weightC1,biasC1, im, weightF1, biasF1, weightF2, biasF2, l, acc, label, out= train(im, label,weightC1, biasc1, weightF1, weightF2, biasF2 )\n",
    "  prediksi_train.append(np.argmax(out))\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "num_tests = len(train_images)\n",
    "print('Train Loss:', loss / num_tests)\n",
    "print('Train Accuracy:', num_correct / num_tests)\n",
    "print(\"prediksi train: \", prediksi_train)\n",
    "\n",
    "\n",
    "# Test the CNN\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "prediksi = []\n",
    "for im, label in zip(test_images, test_labels):\n",
    "  weightC1,biasC1, im, weightF1, biasF1, weightF2, biasF2, l, acc, label, out = train(im, label,weightC1, biasc1, weightF1, weightF2, biasF2 )\n",
    "\n",
    "  prediksi.append(np.argmax(out))\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "num_tests = len(test_images)\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)\n",
    "print(\"Hasil prediksi = \", prediksi,\"\\n\",\n",
    "      \"jumlah datatest = \", len(prediksi),\"\\n\",\n",
    "      \"\\n\")\n",
    "\n",
    "\n",
    "ground_truth = []\n",
    "for i in test_labels :\n",
    "  ground_truth.append(i)\n",
    "print(\"isi ground_truth = \", ground_truth,\"\\n\",\n",
    "      \"jumlah ground truth = \", len(ground_truth),\"\\n\",\n",
    "      \"\\n\")\n",
    "\n",
    "\n",
    "dict = {'Ground_truth':ground_truth, \"Prediksi\":prediksi} \n",
    "df = pd.DataFrame(list(zip(ground_truth, prediksi)), columns = ['Ground_thruth','Prediksi'] )\n",
    "    \n",
    "# saving the dataframe \n",
    "df.to_csv('predict_Run_code_1_z_score.csv') \n",
    "cvsDataframe = pd.read_csv('predict_Run_code_1_z_score.csv')\n",
    "\n",
    "# creating an output excel file\n",
    "\n",
    "cvsDataframe.to_excel (r'predict_ground_truth_1_z_score.xlsx', index = None, header=True)\n",
    "\n",
    "all_loss_concate = np.concatenate(all_loss)\n",
    "print(\"loss minimum : \", all_loss_concate.min(), \"index : \", np.argmin(all_loss_concate))\n",
    "print(all_loss_concate)\n",
    "plt.plot(all_loss_concate)\n",
    "plt.title(\"Grafik Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "all_acc_concate = np.concatenate(all_acc)\n",
    "print(\"Akurasi maksimum : \", all_acc_concate.max(), \"index : \", np.argmax(all_acc_concate))\n",
    "print(all_acc_concate)\n",
    "plt.plot(all_acc_concate)\n",
    "plt.title(\"Grafik Akurasi\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dict = {'All_loss':all_loss_concate, \"All_predict\":all_acc_concate} \n",
    "df = pd.DataFrame(list(zip(all_loss_concate, all_acc_concate)), columns = ['All_loss','All_acc'] )\n",
    "    \n",
    "# saving the dataframe \n",
    "df.to_csv('Loss_Acc_Run_code_1_z_score.csv') \n",
    "cvsDataframe = pd.read_csv('Loss_Acc_Run_code_1_z_score.csv')\n",
    "\n",
    "# creating an output excel file\n",
    "\n",
    "cvsDataframe.to_excel (r'Loss_Acc_1_z_score.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed3 (image, label,weightC1, biasc1, weightC2, biasC2 weightF1, biasF  ):\n",
    "    conv1, wc1, b1, X1 = convolusi(image, weightC1, biasC1, \"no\")\n",
    "    reLuu = reLu(conv1)\n",
    "    maxPool = maxPooling(reLuu,2,2)\n",
    "    conv2, wc2, b2, X2 = convolusi(maxPool, weightC2, biasC2, \"SAME\" )\n",
    "    reLuu2 = reLu(conv2)\n",
    "    maxPool2 = maxPooling(reLuu2,2,2)\n",
    "    flattening = flat(maxPool2)\n",
    "    fc1, wf1, bf1 = hidden_layer(flattening,weightF1, biasF)\n",
    "    out = softmax(fc1)\n",
    "    loss = -np.log(out[label])\n",
    "    prediction = np.argmax(out, 0)\n",
    "    acc = np.sum(prediction == label) / label.size\n",
    "    return wc1, b1, X1, wc2, b2, X2, fc1, dout, wf1, bf1, flattening, reLuu, acc, loss, out\n",
    "\n",
    "\n",
    "def train (wc1, b1, X1, wc2, b2, X2, fc1, dout, bf1, wf1, flattening, reLuu, acc, loss, out) :\n",
    "    dout = np.zeros(20)\n",
    "    dout[label] = -1 / out[label]\n",
    "    dSoftmax = backprop1(fc1, dout)\n",
    "    weightF_update, biasF_update  = backprop2(dSoftmax, wf1, bf1, flattening, lr= 0.005)\n",
    "    Z1 = backMaxPooling(reLuu2, 2, 2)\n",
    "    weightC2, biasC2, X2 = backConvolusi(Z1, wc1, b1, X1, lr=0.005)\n",
    "    Z = backMaxPooling(reLuu, 2, 2)\n",
    "    weightC1, biasC1, image = backConvolusi(Z, wc1, b1, X1, lr=0.005)\n",
    "    \n",
    "    return weightC1,biasC1, image,weightC2, biasC2, X2, weightF_update, biasF_update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
